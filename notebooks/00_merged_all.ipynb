{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 data exploration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration for CausalShapGNN\n",
        "\n",
        "This notebook explores the benchmark datasets used for evaluating CausalShapGNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "from data import DataDownloader, DataPreprocessor\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Download and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset (e.g., MovieLens-100K for quick exploration)\n",
        "downloader = DataDownloader('../data')\n",
        "downloader.download('movielens-100k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess\n",
        "preprocessor = DataPreprocessor('../data', 'movielens-100k')\n",
        "graph_data = preprocessor.load_data()\n",
        "\n",
        "print(f\"Users: {graph_data.n_users}\")\n",
        "print(f\"Items: {graph_data.n_items}\")\n",
        "print(f\"Training interactions: {len(graph_data.train_interactions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Analyze Interaction Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User degree distribution\n",
        "user_degrees = defaultdict(int)\n",
        "item_degrees = defaultdict(int)\n",
        "\n",
        "for u, i in graph_data.train_interactions:\n",
        "    user_degrees[u] += 1\n",
        "    item_degrees[i] += 1\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(list(user_degrees.values()), bins=50, alpha=0.7)\n",
        "axes[0].set_xlabel('Number of Interactions')\n",
        "axes[0].set_ylabel('Number of Users')\n",
        "axes[0].set_title('User Interaction Distribution')\n",
        "axes[0].set_yscale('log')\n",
        "\n",
        "axes[1].hist(list(item_degrees.values()), bins=50, alpha=0.7, color='orange')\n",
        "axes[1].set_xlabel('Number of Interactions')\n",
        "axes[1].set_ylabel('Number of Items')\n",
        "axes[1].set_title('Item Popularity Distribution')\n",
        "axes[1].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compute Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats = preprocessor.compute_statistics(graph_data)\n",
        "\n",
        "for key, value in stats.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Popularity Bias Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lorenz curve for item popularity\n",
        "item_pops = sorted(item_degrees.values())\n",
        "cumsum = np.cumsum(item_pops)\n",
        "cumsum = cumsum / cumsum[-1]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(np.linspace(0, 1, len(cumsum)), cumsum, label='Lorenz Curve')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfect Equality')\n",
        "plt.xlabel('Cumulative Share of Items')\n",
        "plt.ylabel('Cumulative Share of Interactions')\n",
        "plt.title(f'Item Popularity Inequality (Gini = {stats[\"item_gini\"]:.3f})')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 training demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CausalShapGNN Training Demo\n",
        "\n",
        "This notebook demonstrates how to train CausalShapGNN on a benchmark dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from config import get_default_config\n",
        "from data import DataPreprocessor, BipartiteGraphProcessor, RecommendationDataset, collate_fn\n",
        "from models import CausalShapGNN\n",
        "from trainers import Trainer\n",
        "from utils import set_seed\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor = DataPreprocessor('../data', 'movielens-100k')\n",
        "graph_data = preprocessor.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = get_default_config()\n",
        "\n",
        "config['n_users'] = graph_data.n_users\n",
        "config['n_items'] = graph_data.n_items\n",
        "config['embed_dim'] = 64\n",
        "config['n_factors'] = 4\n",
        "config['n_layers'] = 3\n",
        "\n",
        "config['training'] = {\n",
        "    'lr': 0.001,\n",
        "    'batch_size': 1024,\n",
        "    'n_epochs': 50\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in config.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Model and Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process graph\n",
        "graph_processor = BipartiteGraphProcessor(\n",
        "    graph_data.n_users, graph_data.n_items,\n",
        "    graph_data.train_interactions, device\n",
        ")\n",
        "\n",
        "# Create data loader\n",
        "train_dataset = RecommendationDataset(graph_processor, graph_data.train_interactions)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "model = CausalShapGNN(config, device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(model, graph_processor, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_recalls = []\n",
        "\n",
        "n_epochs = config['training']['n_epochs']\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Train\n",
        "    losses = trainer.train_epoch(train_loader, graph_processor.norm_adj)\n",
        "    train_losses.append(losses['total'])\n",
        "    \n",
        "    # Evaluate every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        val_metrics = trainer.evaluate(\n",
        "            graph_processor.norm_adj,\n",
        "            graph_data.val_interactions\n",
        "        )\n",
        "        val_recalls.append(val_metrics['recall@20'])\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
        "        print(f\"  Loss: {losses['total']:.4f}\")\n",
        "        print(f\"  Val R@20: {val_metrics['recall@20']:.4f}\")\n",
        "        print(f\"  Val N@20: {val_metrics['ndcg@20']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Plot Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(train_losses)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss')\n",
        "\n",
        "axes[1].plot(range(5, n_epochs+1, 5), val_recalls, marker='o')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Recall@20')\n",
        "axes[1].set_title('Validation Recall')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_metrics = trainer.evaluate(\n",
        "    graph_processor.norm_adj,\n",
        "    graph_data.test_interactions\n",
        ")\n",
        "\n",
        "print(\"\\nTest Set Results:\")\n",
        "for k, v in sorted(test_metrics.items()):\n",
        "    print(f\"  {k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 explanation demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CausalShapGNN Explanation Demo\n",
        "\n",
        "This notebook demonstrates the multi-granularity explanations generated by CausalShapGNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from config import get_default_config\n",
        "from data import DataPreprocessor, BipartiteGraphProcessor\n",
        "from models import CausalShapGNN\n",
        "from explainers import FeatureShapley, PathShapley, UserProfileShapley\n",
        "from explainers import ExplanationReport, ExplanationVisualizer\n",
        "from utils import set_seed\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "preprocessor = DataPreprocessor('../data', 'movielens-100k')\n",
        "graph_data = preprocessor.load_data()\n",
        "\n",
        "# Setup config\n",
        "config = get_default_config()\n",
        "config['n_users'] = graph_data.n_users\n",
        "config['n_items'] = graph_data.n_items\n",
        "config['embed_dim'] = 64\n",
        "config['n_factors'] = 4\n",
        "config['n_layers'] = 3\n",
        "\n",
        "# Process graph\n",
        "graph_processor = BipartiteGraphProcessor(\n",
        "    graph_data.n_users, graph_data.n_items,\n",
        "    graph_data.train_interactions, device\n",
        ")\n",
        "\n",
        "# Initialize model (in practice, load trained checkpoint)\n",
        "model = CausalShapGNN(config, device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_id = 42\n",
        "\n",
        "with torch.no_grad():\n",
        "    user_emb, item_emb, _ = model(graph_processor.norm_adj, use_causal_only=True)\n",
        "\n",
        "scores = torch.matmul(user_emb[user_id], item_emb.t())\n",
        "\n",
        "# Mask training items\n",
        "train_items = list(graph_processor.train_user_items[user_id])\n",
        "if train_items:\n",
        "    scores[train_items] = -float('inf')\n",
        "\n",
        "_, top_items = torch.topk(scores, 10)\n",
        "top_items = top_items.cpu().numpy().tolist()\n",
        "\n",
        "print(f\"Top 10 recommendations for User {user_id}:\")\n",
        "for i, item in enumerate(top_items):\n",
        "    print(f\"  {i+1}. Item {item}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature-Level Explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_explainer = FeatureShapley(model, device)\n",
        "feature_explainer._compute_population_means(user_emb, item_emb)\n",
        "\n",
        "item_idx = top_items[0]\n",
        "shapley = feature_explainer.compute(user_id, item_idx, user_emb, item_emb)\n",
        "\n",
        "factor_names = ['Genre', 'Recency', 'Quality', 'Social']\n",
        "\n",
        "print(f\"\\nFeature-level explanation for Item {item_idx}:\")\n",
        "for name, value in zip(factor_names, shapley):\n",
        "    print(f\"  {name}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize\n",
        "visualizer = ExplanationVisualizer(factor_names)\n",
        "\n",
        "colors = ['green' if v >= 0 else 'red' for v in shapley]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.barh(factor_names, shapley, color=colors)\n",
        "plt.xlabel('Shapley Value')\n",
        "plt.title(f'Factor Contributions for Item {item_idx}')\n",
        "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. User Profile Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "profile_explainer = UserProfileShapley(feature_explainer)\n",
        "user_profile = profile_explainer.compute(user_id, top_items, user_emb, item_emb)\n",
        "\n",
        "report_generator = ExplanationReport(model, device, factor_names)\n",
        "report = report_generator.generate_user_profile_report(user_id, user_profile, top_items)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare Explanations Across Items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get explanations for top 5 items\n",
        "explanations = []\n",
        "for item_idx in top_items[:5]:\n",
        "    shapley = feature_explainer.compute(user_id, item_idx, user_emb, item_emb)\n",
        "    explanations.append({'item_idx': item_idx, 'feature_shapley': shapley})\n",
        "\n",
        "# Create heatmap\n",
        "shapley_matrix = np.array([e['feature_shapley'] for e in explanations])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(shapley_matrix, cmap='RdBu_r', aspect='auto')\n",
        "plt.colorbar(label='Shapley Value')\n",
        "plt.xticks(range(len(factor_names)), factor_names)\n",
        "plt.yticks(range(len(explanations)), [f\"Item {e['item_idx']}\" for e in explanations])\n",
        "plt.title('Factor Contributions Across Recommendations')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}